{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder for the input image\n",
    "        self.encoder_img = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Encoder for the wind data\n",
    "        self.encoder_wind = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder to generate the next frame\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, wind):\n",
    "        img_encoded = self.encoder_img(img)\n",
    "        wind_encoded = self.encoder_wind(wind)\n",
    "        combined = torch.cat((img_encoded, wind_encoded), dim=1)\n",
    "        decoded = self.decoder(combined)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     58\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 59\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[1;34m(self, data_dir)\u001b[0m\n\u001b[0;32m     13\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(day_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_batches\u001b[38;5;241m.\u001b[39mappend([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(day_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m image_files])\n\u001b[1;32m---> 16\u001b[0m wind_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m wind_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(day_path, wind_data_file)\n\u001b[0;32m     18\u001b[0m wind_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(wind_data_path)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.day_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])\n",
    "        \n",
    "        self.image_batches = []\n",
    "        self.wind_data_batches = []\n",
    "        \n",
    "\n",
    "      \n",
    "        for day_folder in self.day_folders:\n",
    "            day_path = os.path.join(data_dir, day_folder)\n",
    "            image_files = sorted([f for f in os.listdir(day_path) if f.endswith('.jpg')])\n",
    "            self.image_batches.append([os.path.join(day_path, f) for f in image_files])\n",
    "            \n",
    "            wind_data_file = sorted([f for f in os.listdir(day_path) if f.endswith('.npy')])[0]\n",
    "            wind_data_path = os.path.join(day_path, wind_data_file)\n",
    "            wind_data = np.load(wind_data_path)\n",
    "            \n",
    "            # Ensure wind data is in the correct format (H, W, 3)\n",
    "           \n",
    "\n",
    "            if wind_data.ndim == 3 and wind_data.shape[2] == 3:\n",
    "                self.wind_data_batches.append(wind_data)\n",
    "            else:\n",
    "                raise ValueError(f\"Wind data in {day_path} is not in the correct format.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Total number of image pairs across all days\n",
    "        return sum(len(images) - 1 for images in self.image_batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which day folder and image pair the index refers to\n",
    "        running_idx = 0\n",
    "        for images, wind_data in zip(self.image_batches, self.wind_data_batches):\n",
    "            num_images = len(images) - 1\n",
    "            if idx < running_idx + num_images:\n",
    "                img1_path = images[idx - running_idx]\n",
    "                img2_path = images[idx - running_idx + 1]\n",
    "                wind_data = np.repeat(wind_data[np.newaxis, :, :, :], 1, axis=0)  # Adjust to match batch size\n",
    "\n",
    "                # Load and preprocess images\n",
    "                img1 = np.array(Image.open(img1_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                img2 = np.array(Image.open(img2_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                \n",
    "                # Convert to torch tensors\n",
    "                img1 = torch.tensor(img1.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                img2 = torch.tensor(img2.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                wind_data = torch.tensor(wind_data.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                \n",
    "                return img1, img2, wind_data\n",
    "\n",
    "            running_idx += num_images\n",
    "\n",
    "        raise IndexError(\"Index out of range\")\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'output'\n",
    "train_dataset = CustomDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     65\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 66\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[1;34m(self, data_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(day_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_batches\u001b[38;5;241m.\u001b[39mappend([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(day_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m image_files])\n\u001b[1;32m---> 20\u001b[0m wind_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m wind_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(day_path, wind_data_file)\n\u001b[0;32m     22\u001b[0m wind_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(wind_data_path)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.day_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])\n",
    "        \n",
    "        self.image_batches = []\n",
    "        self.wind_data_batches = []\n",
    "\n",
    "        for day_folder in self.day_folders:\n",
    "            day_path = os.path.join(data_dir, day_folder)\n",
    "            image_files = sorted([f for f in os.listdir(day_path) if f.endswith('.jpg')])\n",
    "            self.image_batches.append([os.path.join(day_path, f) for f in image_files])\n",
    "            \n",
    "            wind_data_file = sorted([f for f in os.listdir(day_path) if f.endswith('.npy')])[0]\n",
    "            wind_data_path = os.path.join(day_path, wind_data_file)\n",
    "            wind_data = np.load(wind_data_path)\n",
    "            \n",
    "            # Ensure wind data is in the correct format (H, W, 3)\n",
    "            if wind_data.ndim == 3 and wind_data.shape[2] == 3:\n",
    "                self.wind_data_batches.append(wind_data)\n",
    "            else:\n",
    "                raise ValueError(f\"Wind data in {day_path} is not in the correct format.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Total number of image pairs across all days\n",
    "        return sum(len(images) - 1 for images in self.image_batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which day folder and image pair the index refers to\n",
    "        running_idx = 0\n",
    "        for images, wind_data in zip(self.image_batches, self.wind_data_batches):\n",
    "            num_images = len(images) - 1\n",
    "            if idx < running_idx + num_images:\n",
    "                img1_path = images[idx - running_idx]\n",
    "                img2_path = images[idx - running_idx + 1]\n",
    "\n",
    "                # Load and preprocess images\n",
    "                img1 = np.array(Image.open(img1_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                img2 = np.array(Image.open(img2_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                \n",
    "                # Convert to torch tensors\n",
    "                img1 = torch.tensor(img1.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                img2 = torch.tensor(img2.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                \n",
    "                # Select a single frame of wind data corresponding to the image pair\n",
    "                wind_data_frame = wind_data[:, :, :2]  # Use x and y components\n",
    "                wind_data_frame = np.expand_dims(wind_data_frame, axis=0)  # Add batch dimension\n",
    "\n",
    "                # Convert wind data to torch tensor\n",
    "                wind_data_tensor = torch.tensor(wind_data_frame.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                \n",
    "                return img1, img2, wind_data_tensor\n",
    "\n",
    "            running_idx += num_images\n",
    "\n",
    "        raise IndexError(\"Index out of range\")\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'output'\n",
    "train_dataset = CustomDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scipy) (1.26.4)\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 12.6/44.5 MB 65.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.0/44.5 MB 63.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.7/44.5 MB 63.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.4/44.5 MB 51.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 47.2 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_size=(512, 512)):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = image_size\n",
    "        self.day_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.wind_data_batches = []\n",
    "\n",
    "        # Load all images and wind data files\n",
    "        for day_folder in self.day_folders:\n",
    "            day_path = os.path.join(data_dir, day_folder)\n",
    "            image_files = sorted([f for f in os.listdir(day_path) if f.endswith('.jpg')])\n",
    "            self.image_paths.extend([os.path.join(day_path, f) for f in image_files])\n",
    "            \n",
    "            wind_data_file = sorted([f for f in os.listdir(day_path) if f.endswith('.npy')])[0]\n",
    "            wind_data_path = os.path.join(day_path, wind_data_file)\n",
    "            wind_data = np.load(wind_data_path)\n",
    "            \n",
    "            # Ensure wind data is resized to match the image size\n",
    "            if wind_data.shape[:2] != self.image_size:\n",
    "                # Resize using scipy\n",
    "                zoom_factors = (self.image_size[0] / wind_data.shape[0], \n",
    "                                self.image_size[1] / wind_data.shape[1])\n",
    "                resized_wind_data = zoom(wind_data, (zoom_factors[0], zoom_factors[1], 1), order=1)\n",
    "                wind_data = resized_wind_data\n",
    "            \n",
    "            if wind_data.ndim == 3 and wind_data.shape[2] == 3:\n",
    "                self.wind_data_batches.append(wind_data)\n",
    "            else:\n",
    "                raise ValueError(f\"Wind data in {day_path} is not in the correct format. Expected shape (H, W, 3), got {wind_data.shape}.\")\n",
    "        \n",
    "        # Flatten wind_data_batches to ensure proper indexing\n",
    "        self.wind_data_batches = [wind_data for wind_data in self.wind_data_batches for _ in range(len(self.image_paths) // len(self.wind_data_batches))]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of image pairs across all days\n",
    "        return len(self.image_paths) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        \n",
    "        # Determine which image pair and corresponding wind data\n",
    "        img1_path = self.image_paths[idx]\n",
    "        img2_path = self.image_paths[idx + 1]\n",
    "        \n",
    "        # Load and preprocess images\n",
    "        img1 = np.array(Image.open(img1_path).resize(self.image_size).convert('RGB')) / 255.0\n",
    "        img2 = np.array(Image.open(img2_path).resize(self.image_size).convert('RGB')) / 255.0\n",
    "        \n",
    "        # Choose the appropriate wind data\n",
    "        wind_data = self.wind_data_batches[idx % len(self.wind_data_batches)]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        img1 = torch.tensor(img1.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        img2 = torch.tensor(img2.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        \n",
    "        try:\n",
    "            wind_data_tensor = torch.tensor(wind_data.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error transposing wind data: {e}\")\n",
    "            print(f\"Wind data shape before transpose: {wind_data.shape}\")\n",
    "            raise\n",
    "        \n",
    "        return img1, img2, wind_data_tensor\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'output(with wind)'\n",
    "train_dataset = CustomDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for img1, img2, wind in train_loader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        wind = wind.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img1, wind)\n",
    "        loss = criterion(outputs, img2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'D:/trained_autoencoder.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
