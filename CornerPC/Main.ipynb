{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder for the input image\n",
    "        self.encoder_img = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Encoder for the wind data\n",
    "        self.encoder_wind = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder to generate the next frame\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, wind):\n",
    "        img_encoded = self.encoder_img(img)\n",
    "        wind_encoded = self.encoder_wind(wind)\n",
    "        combined = torch.cat((img_encoded, wind_encoded), dim=1)\n",
    "        decoded = self.decoder(combined)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.day_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])\n",
    "        \n",
    "        self.image_batches = []\n",
    "        self.wind_data_batches = []\n",
    "        \n",
    "\n",
    "      \n",
    "        for day_folder in self.day_folders:\n",
    "            day_path = os.path.join(data_dir, day_folder)\n",
    "            image_files = sorted([f for f in os.listdir(day_path) if f.endswith('.jpg')])\n",
    "            self.image_batches.append([os.path.join(day_path, f) for f in image_files])\n",
    "            \n",
    "            wind_data_file = sorted([f for f in os.listdir(day_path) if f.endswith('.npy')])[0]\n",
    "            wind_data_path = os.path.join(day_path, wind_data_file)\n",
    "            wind_data = np.load(wind_data_path)\n",
    "            \n",
    "            # Ensure wind data is in the correct format (H, W, 3)\n",
    "           \n",
    "\n",
    "            if wind_data.ndim == 3 and wind_data.shape[2] == 3:\n",
    "                self.wind_data_batches.append(wind_data)\n",
    "            else:\n",
    "                raise ValueError(f\"Wind data in {day_path} is not in the correct format.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Total number of image pairs across all days\n",
    "        return sum(len(images) - 1 for images in self.image_batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which day folder and image pair the index refers to\n",
    "        running_idx = 0\n",
    "        for images, wind_data in zip(self.image_batches, self.wind_data_batches):\n",
    "            num_images = len(images) - 1\n",
    "            if idx < running_idx + num_images:\n",
    "                img1_path = images[idx - running_idx]\n",
    "                img2_path = images[idx - running_idx + 1]\n",
    "                wind_data = np.repeat(wind_data[np.newaxis, :, :, :], 1, axis=0)  # Adjust to match batch size\n",
    "\n",
    "                # Load and preprocess images\n",
    "                img1 = np.array(Image.open(img1_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                img2 = np.array(Image.open(img2_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                \n",
    "                # Convert to torch tensors\n",
    "                img1 = torch.tensor(img1.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                img2 = torch.tensor(img2.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                wind_data = torch.tensor(wind_data.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                \n",
    "                return img1, img2, wind_data\n",
    "\n",
    "            running_idx += num_images\n",
    "\n",
    "        raise IndexError(\"Index out of range\")\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'output'\n",
    "train_dataset = CustomDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.day_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])\n",
    "        \n",
    "        self.image_batches = []\n",
    "        self.wind_data_batches = []\n",
    "\n",
    "        for day_folder in self.day_folders:\n",
    "            day_path = os.path.join(data_dir, day_folder)\n",
    "            image_files = sorted([f for f in os.listdir(day_path) if f.endswith('.jpg')])\n",
    "            self.image_batches.append([os.path.join(day_path, f) for f in image_files])\n",
    "            \n",
    "            wind_data_file = sorted([f for f in os.listdir(day_path) if f.endswith('.npy')])[0]\n",
    "            wind_data_path = os.path.join(day_path, wind_data_file)\n",
    "            wind_data = np.load(wind_data_path)\n",
    "            \n",
    "            # Ensure wind data is in the correct format (H, W, 3)\n",
    "            if wind_data.ndim == 3 and wind_data.shape[2] == 3:\n",
    "                self.wind_data_batches.append(wind_data)\n",
    "            else:\n",
    "                raise ValueError(f\"Wind data in {day_path} is not in the correct format.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Total number of image pairs across all days\n",
    "        return sum(len(images) - 1 for images in self.image_batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which day folder and image pair the index refers to\n",
    "        running_idx = 0\n",
    "        for images, wind_data in zip(self.image_batches, self.wind_data_batches):\n",
    "            num_images = len(images) - 1\n",
    "            if idx < running_idx + num_images:\n",
    "                img1_path = images[idx - running_idx]\n",
    "                img2_path = images[idx - running_idx + 1]\n",
    "\n",
    "                # Load and preprocess images\n",
    "                img1 = np.array(Image.open(img1_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                img2 = np.array(Image.open(img2_path).resize((512, 512)).convert('RGB')) / 255.0\n",
    "                \n",
    "                # Convert to torch tensors\n",
    "                img1 = torch.tensor(img1.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                img2 = torch.tensor(img2.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                \n",
    "                # Select a single frame of wind data corresponding to the image pair\n",
    "                wind_data_frame = wind_data[:, :, :2]  # Use x and y components\n",
    "                wind_data_frame = np.expand_dims(wind_data_frame, axis=0)  # Add batch dimension\n",
    "\n",
    "                # Convert wind data to torch tensor\n",
    "                wind_data_tensor = torch.tensor(wind_data_frame.transpose(2, 0, 1), dtype=torch.float32)\n",
    "                \n",
    "                return img1, img2, wind_data_tensor\n",
    "\n",
    "            running_idx += num_images\n",
    "\n",
    "        raise IndexError(\"Index out of range\")\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'output'\n",
    "train_dataset = CustomDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scipy) (1.26.4)\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 11.8/44.5 MB 56.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 24.1/44.5 MB 61.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.7/44.5 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 58.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 53.5 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.14.1\n"
     ]
    }
   ],
   "source": [
    "pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_size=(512, 512)):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = image_size\n",
    "        self.day_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.wind_data_batches = []\n",
    "\n",
    "        # Load all images and wind data files\n",
    "        for day_folder in self.day_folders:\n",
    "            day_path = os.path.join(data_dir, day_folder)\n",
    "            image_files = sorted([f for f in os.listdir(day_path) if f.endswith('.jpg')])\n",
    "            self.image_paths.extend([os.path.join(day_path, f) for f in image_files])\n",
    "            \n",
    "            wind_data_file = sorted([f for f in os.listdir(day_path) if f.endswith('.npy')])[0]\n",
    "            wind_data_path = os.path.join(day_path, wind_data_file)\n",
    "            wind_data = np.load(wind_data_path)\n",
    "            \n",
    "            # Ensure wind data is resized to match the image size\n",
    "            if wind_data.shape[:2] != self.image_size:\n",
    "                # Resize using scipy\n",
    "                zoom_factors = (self.image_size[0] / wind_data.shape[0], \n",
    "                                self.image_size[1] / wind_data.shape[1])\n",
    "                resized_wind_data = zoom(wind_data, (zoom_factors[0], zoom_factors[1], 1), order=1)\n",
    "                wind_data = resized_wind_data\n",
    "            \n",
    "            if wind_data.ndim == 3 and wind_data.shape[2] == 3:\n",
    "                self.wind_data_batches.append(wind_data)\n",
    "            else:\n",
    "                raise ValueError(f\"Wind data in {day_path} is not in the correct format. Expected shape (H, W, 3), got {wind_data.shape}.\")\n",
    "        \n",
    "        # Flatten wind_data_batches to ensure proper indexing\n",
    "        self.wind_data_batches = [wind_data for wind_data in self.wind_data_batches for _ in range(len(self.image_paths) // len(self.wind_data_batches))]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of image pairs across all days\n",
    "        return len(self.image_paths) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        \n",
    "        # Determine which image pair and corresponding wind data\n",
    "        img1_path = self.image_paths[idx]\n",
    "        img2_path = self.image_paths[idx + 1]\n",
    "        \n",
    "        # Load and preprocess images\n",
    "        img1 = np.array(Image.open(img1_path).resize(self.image_size).convert('RGB')) / 255.0\n",
    "        img2 = np.array(Image.open(img2_path).resize(self.image_size).convert('RGB')) / 255.0\n",
    "        \n",
    "        # Choose the appropriate wind data\n",
    "        wind_data = self.wind_data_batches[idx % len(self.wind_data_batches)]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        img1 = torch.tensor(img1.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        img2 = torch.tensor(img2.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        \n",
    "        try:\n",
    "            wind_data_tensor = torch.tensor(wind_data.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error transposing wind data: {e}\")\n",
    "            print(f\"Wind data shape before transpose: {wind_data.shape}\")\n",
    "            raise\n",
    "        \n",
    "        return img1, img2, wind_data_tensor\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'output'\n",
    "train_dataset = CustomDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.49345852539572915\n",
      "Epoch [2/20], Loss: 0.4905348751746433\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for img1, img2, wind in train_loader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        wind = wind.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img1, wind)\n",
    "        loss = criterion(outputs, img2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'D:/trained_autoencoder.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
